{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dataset Creation Jupyter Notebook\n",
    "This is where the code for the processing of data and the creation of the dataset for phenophase classification will reside.\n",
    "\n",
    "Two important parts of this: \n",
    "(1) finding average transition dates for each site and each year\n",
    "(2) storing this data in a readable JSON format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def date_to_doy(date):\n",
    "    year = int(date[:date.find('-')])\n",
    "    adjusted_input = date[date.find('-')+1:]\n",
    "    month = int(adjusted_input[:adjusted_input.find('-')])\n",
    "    day = int(adjusted_input[adjusted_input.find('-')+1:])\n",
    "\n",
    "    dates_in_prev_months = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334] #index 0 is jan, 1 is feb etc.\n",
    "    doy = dates_in_prev_months[month - 1] + day\n",
    "    \n",
    "    if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0) and month > 2:\n",
    "        doy += 1\n",
    "\n",
    "    return doy, year\n",
    "\n",
    "def doy_to_date(doy, year):\n",
    "    leap_year = year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "    dates_in_prev_months = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334] #index 0 is jan, 1 is feb etc.\n",
    "    dates_in_prev_months_lyr = [0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
    "\n",
    "    month = 0\n",
    "    day = 1\n",
    "\n",
    "    for i in range(len(dates_in_prev_months)):\n",
    "        if leap_year:\n",
    "            if i == 11:\n",
    "                month = 12\n",
    "                day = doy - dates_in_prev_months_lyr[i]\n",
    "                return year, month, day\n",
    "\n",
    "            elif doy > dates_in_prev_months_lyr[i] and doy <= dates_in_prev_months_lyr[i+1]:\n",
    "                month = i + 1\n",
    "                if month >= 2:\n",
    "                    day = doy - dates_in_prev_months_lyr[i]\n",
    "                else: \n",
    "                    day = doy - dates_in_prev_months[i]\n",
    "                return year, month, day\n",
    "        else:\n",
    "            if i == 11:\n",
    "                month = 12\n",
    "                day = doy - dates_in_prev_months[i]\n",
    "                return year, month, day\n",
    "            elif doy > dates_in_prev_months[i] and doy <= dates_in_prev_months[i+1]:\n",
    "                month = i + 1\n",
    "                day = doy - dates_in_prev_months[i]\n",
    "                return year, month, day\n",
    "        \n",
    "    return year, month, day\n",
    "\n",
    "def calc_average_transition_date(str_dates_list, is_rising):\n",
    "    years = []\n",
    "    doys = []\n",
    "    for string in str_dates_list:\n",
    "        doy, year = date_to_doy(string)\n",
    "        doys.append(doy)\n",
    "        years.append(year)\n",
    "    avg_year = int(statistics.median(years))\n",
    "    avg_doy = int(statistics.median(doys))\n",
    "    _, avg_month, avg_day = doy_to_date(avg_doy, avg_year)\n",
    "    date = str(avg_year) + '_' + str(avg_month) + '_' + str(avg_day)\n",
    "\n",
    "    return {'rising':is_rising, 'date':date , 'year':avg_year, 'month':avg_month, 'day':avg_day, 'doy':avg_doy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract all transition dates from CSV files, then caculate median date, and save the respective transition dates for each to a JSON file\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "directory = \"./phenocam_data/\"\n",
    "files_in_directory = os.listdir(directory)\n",
    "filtered_files = [file for file in files_in_directory if file.endswith(\".csv\")]\n",
    "\n",
    "for file in filtered_files:\n",
    "    path_to_file = os.path.join(directory, file)\n",
    "    sitename = file[:file.find('_')]\n",
    "\n",
    "    df = pd.read_csv(path_to_file, index_col=False)\n",
    "    df_lists = df.values.tolist()\n",
    "    \n",
    "    num_rising = 0\n",
    "    num_falling = 0\n",
    "    for list in df_lists:\n",
    "        if 'rising' in (string for string in list): \n",
    "            num_rising += 1\n",
    "        elif 'falling' in (string for string in list): \n",
    "            num_falling += 1 \n",
    "\n",
    "    num_rising_transitions = int(num_rising / 4)\n",
    "    num_falling_transitions = int(num_falling / 4)\n",
    "\n",
    "    rising = [[] for _ in range(num_rising_transitions)]\n",
    "    falling = [[] for _ in range(num_falling_transitions)]\n",
    "\n",
    "    i = 0\n",
    "    for list in df_lists:\n",
    "        if 'rising' in (string for string in list): \n",
    "            rising[i].extend(list[5:14])\n",
    "            i += 1\n",
    "            if i==num_rising_transitions: i=0\n",
    "        elif 'falling' in (string for string in list): \n",
    "            falling[i].extend(list[5:14])  \n",
    "            i += 1\n",
    "            if i==num_falling_transitions: i=0\n",
    "    \n",
    "    avg_transitions = [calc_average_transition_date(transition, True) for transition in rising]\n",
    "    avg_transitions.extend([calc_average_transition_date(transition, False) for transition in falling])\n",
    "\n",
    "    transition_date_data = {\n",
    "        'sitename': sitename,\n",
    "        'transitions': avg_transitions,\n",
    "    }\n",
    "\n",
    "    file_to_save = sitename + '_transition_dates.json'\n",
    "    path_to_target = os.path.join(directory, file_to_save) \n",
    "    with open(path_to_target, 'w') as f:\n",
    "        json.dump(transition_date_data, f, indent=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method, given input of img_file, output boolean indicating if rising or falling\n",
    "def is_rising(filename):\n",
    "    truncated_filename = filename[filename.rfind('/')+1:]\n",
    "    sitename = truncated_filename[:truncated_filename.find('_')]\n",
    "    date = truncated_filename[truncated_filename.find('_')+1:-11].replace('_', '-') \n",
    "    doy, year = date_to_doy(date)\n",
    "    total_days = 365.2422 * year + doy\n",
    "\n",
    "    # load json transition dates file\n",
    "    with open('./phenocam_data/' + sitename + '_transition_dates.json', 'r') as file:\n",
    "         site_transitions =  json.load(file)['transitions']\n",
    "\n",
    "    # find closest\n",
    "    distances = []\n",
    "    for i in range(len(site_transitions)):\n",
    "        year_trans = site_transitions[i]['year']\n",
    "        doy_trans = site_transitions[i]['doy']\n",
    "        total_days_trans = 365.2422 * year_trans + doy_trans\n",
    "        distance = total_days - total_days_trans\n",
    "        abs_distance = abs(distance)\n",
    "        distances.append((i, distance, abs_distance))\n",
    "    \n",
    "    closest = min(distances, key=lambda x: x[2])\n",
    "    if site_transitions[closest[0]]['rising']:\n",
    "        return closest[1] >= 0\n",
    "    else:\n",
    "        return closest[1] < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort images into \"sorted_images\" folder according to rising or falling\n",
    "from PIL import Image\n",
    "directory = \"./phenocam_data/\"\n",
    "target_directory = '../PhenoCam_v2/sorted_images/'\n",
    "\n",
    "files_in_directory = os.listdir(directory)\n",
    "filtered_files = [file for file in files_in_directory if file.endswith(\"imgs.json\")]\n",
    "\n",
    "print(\"Started sorting!\")\n",
    "for file in filtered_files:\n",
    "    print(\"Site:\", file[:-10])\n",
    "    with open(directory + file, 'r') as f:\n",
    "        img_list = json.load(f)['img_file_names']\n",
    "    for img_filename in img_list:\n",
    "        filename = img_filename[img_filename.rfind('/')+1:] \n",
    "        if is_rising(img_filename):\n",
    "            img = Image.open(img_filename)\n",
    "            img.save(target_directory + 'rising/' + filename)\n",
    "        else:\n",
    "            img = Image.open(img_filename)\n",
    "            img.save(target_directory + 'falling/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into data folder as needed for dataset structure\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "directory = '../PhenoCam_v2/sorted_images/'\n",
    "target_directory = './data/'\n",
    "\n",
    "# sort images classified as 'rising'\n",
    "files_in_directory = os.listdir(directory + 'rising')\n",
    "print(files_in_directory[:10])\n",
    "for file in files_in_directory:\n",
    "    im = Image.open(directory + 'rising/' + file)\n",
    "    im.resize((86, 64))\n",
    "    im.save(target_directory + 'rising_' + file)\n",
    "\n",
    "# sort images classified as 'falling'\n",
    "files_in_directory = os.listdir(directory + 'falling')\n",
    "print(files_in_directory[:10])\n",
    "for file in files_in_directory:\n",
    "    im = Image.open(directory + 'falling/' + file)\n",
    "    im.resize((86, 64))\n",
    "    im.save(target_directory + 'falling_' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test cnn to see if it works (using imagedatagenerator)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "# images are of ratio 43:32\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "generator = datagen.flow_from_directory('../PhenoCam_v2/sorted_images/', target_size=(172,128), batch_size=64, shuffle=True, class_mode='binary')\n",
    "\n",
    "#define model (AlexNet)\n",
    "model = Sequential([\n",
    "    Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(172, 128, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy(), metrics=[BinaryAccuracy(threshold=0.5), AUC(), Precision(), Recall()])\n",
    "\n",
    "# train model\n",
    "model.fit(generator, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}